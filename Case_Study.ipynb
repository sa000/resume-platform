{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea8b51e-d122-4b88-bba2-04c5c97bd038",
   "metadata": {},
   "source": [
    "# Data Science Case Study: Candidate Resume Search Platform\n",
    "\n",
    "## Business Context & Objective\n",
    "\n",
    "### Background\n",
    "You are joining the Business Development team at Millennium which is a global hedge fund that manages assets across multiple investment strategies (fundamental equity, systematic trading, credit, etc.). The BD team is responsible for sourcing junior analyst talent across different:\n",
    "\n",
    "- **Geographic Markets**: US, Europe, Asia-Pacific\n",
    "- **Investment Approaches**: Fundamental vs. Systematic/Quantitative strategies\n",
    "- **Sectors**: Technology, Healthcare, Financial Services, Energy, Industrials, Consumer, Credit, Macro, etc.\n",
    "- **Experience Levels**: depending on the job requisitions\n",
    "\n",
    "### Goal\n",
    "Build a **searchable platform** to quickly identify candidates based on specific criteria based on job requisitions.\n",
    "\n",
    "### Your Task\n",
    "1. Parse resume data from PDF/Word documents using **LLM models via API**\n",
    "2. Create parsed resume data as **JSON, CSV, etc.** for further analysis\n",
    "3. Create a **Streamlit web application** where BD users can search and filter candidates using multiple criteria\n",
    "   (Based on the background provided above, okyou will come up with relevant filters)\n",
    "4. Visualize candidate distributions and insights\n",
    "5. **Design for scalability** to handle large volumes of resumes\n",
    "\n",
    "### Output\n",
    "1. Code for data parsing and Streamlit in **this Jupyter notebook** for ease of review\n",
    "2. Include the **link to Streamlit app** in the notebook\n",
    "3. **JSON/CSV exports** of parsed resume data\n",
    "4. Discussion of additional features and implementation approach if more time was available\n",
    "---\n",
    "\n",
    "## Sample Resume Data\n",
    "\n",
    "**You have access to 10 made up resume files representing different candidate profiles**\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Requirements & Evaluation\n",
    "  \n",
    "### Evaluation Criteria\n",
    "1. **Technical Implementation (40%)**: Code quality, data processing, performance\n",
    "2. **User Experience (30%)**: Interface design, functionality, responsiveness\n",
    "3. **Business Value (20%)**: Feature completeness, search capabilities, insights\n",
    "4. **Documentation (10%)**: Code comments, README, presentation\n",
    "\n",
    "### Time Recommendation\n",
    "- **Total Estimated Time**: ~9 hours (**tight but achievable with focused approach**)\n",
    "- **Recommended Strategy**: Build core MVP in 6-7 hours, then enhance\n",
    "\n",
    "### Phased Approach (Recommended)\n",
    "**Phase 1 (3 hours): Core Parsing & Data**\n",
    "- Set up LLM API integration (OpenAI, Anthropic, or alternatives)\n",
    "- Parse 2-3 sample resumes using LLM\n",
    "- Create structured data format with JSON, etc.\n",
    "- Build basic data validation and quality checks\n",
    " \n",
    "**Phase 2 (3 hours): Streamlit**  \n",
    "- Basic filter interface\n",
    "- Simple results display\n",
    "- Core visualizations\n",
    "\n",
    "**Phase 3 (2-3 hours): Enhancement**\n",
    "- Parse remaining resumes\n",
    "- Add advanced filters\n",
    "- Polish UI/UX\n",
    "\n",
    "### Success Criteria (MVP)\n",
    "- LLM-powered parsing of at least 5-7 resume files with high accuracy\n",
    "- Working Streamlit app with basic search functionality\n",
    "- At least 3-4 meaningful filters working\n",
    "- A few visualizations showing candidate insights\n",
    "- JSON/CSV output of parsed resume data\n",
    "- Clean, documented code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "#Parser Utils\n",
    "from utils.parser import extract_text\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "from utils.prompts import RESUME_PARSER_PROMPT, SUMMARY_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAW_DIR = \"data/resumes/raw\"\n",
    "PROCESSED_DIR = \"data/resumes/processed\"\n",
    "SUMMARY_DIR = \"data/resumes/summaries\"\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_candidate(parsed_data: dict, filename: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Summarizes a candidate's resume into a structured JSON object.\n",
    "    Uses parsed resume data to extract key highlights for quick review in the app.\n",
    "\n",
    "    Args:\n",
    "        parsed_data (dict): Parsed resume data from parse_resume().\n",
    "        filename (str): Original filename to help extract candidate name.\n",
    "\n",
    "    Returns:\n",
    "        dict: Structured candidate summary.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    prompt = SUMMARY_PROMPT.format(parsed_data=json.dumps(parsed_data, indent=2), filename=filename)\n",
    "    try:\n",
    "        logger.debug(\"Generating candidate summary...\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data summarization assistant that outputs only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        logger.debug(\"Summary response preview: %s...\", content[:500])\n",
    "\n",
    "        summary_data = json.loads(content)\n",
    "        return summary_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON summary: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LLM summarization failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resume(resume_text: str, filename: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Parses resume text using an LLM prompt optimized for hedge fund BD sourcing.\n",
    "    Returns structured JSON with normalized, business-relevant fields.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Raw text content of the resume\n",
    "        filename (str): Original filename to help extract candidate name\n",
    "\n",
    "    Returns:\n",
    "        dict: Structured resume data with normalized fields\n",
    "    \"\"\"\n",
    "    from utils.prompts import RESUME_PARSER_PROMPT\n",
    "    prompt = RESUME_PARSER_PROMPT.format(resume_text=resume_text, filename=filename)\n",
    "    logger.debug(\"Prompt preview: %s...\", prompt[:500])\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data extraction expert that outputs only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        logger.debug(\"Response preview: %s...\", content[:500])\n",
    "        parsed_data = json.loads(content)\n",
    "        return parsed_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON response: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LLM request failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting resume processing pipeline...\")\n",
    "\n",
    "files = os.listdir(RAW_DIR)\n",
    "logger.info(f\"Processing {len(files)} resume(s)\")\n",
    "\n",
    "for filename in files:\n",
    "    file_path = os.path.join(RAW_DIR, filename)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    parsed_output_path = os.path.join(PROCESSED_DIR, f\"{base_name}_parsed.json\")\n",
    "    summary_output_path = os.path.join(SUMMARY_DIR, f\"{base_name}_summary.json\")\n",
    "\n",
    "    logger.info(f\"Processing: {filename}\")\n",
    "\n",
    "    try:\n",
    "        text = extract_text(file_path)\n",
    "\n",
    "        # Parse the full structured data\n",
    "        parsed_data = parse_resume(text, filename=base_name)\n",
    "        with open(parsed_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved parsed data → {parsed_output_path}\")\n",
    "\n",
    "        # 2Generate summarized profile using parsed data\n",
    "        summary_data = summarize_candidate(parsed_data, filename=base_name)\n",
    "        with open(summary_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved summary → {summary_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(RAW_DIR)[:5]  # Process first file only for testing\n",
    "logger.info(f\"Processing {len(files)} resume(s)\")\n",
    "\n",
    "for filename in files:\n",
    "    file_path = os.path.join(RAW_DIR, filename)\n",
    "    output_filename = filename.rsplit(\".\", 1)[0] + \".json\"\n",
    "    output_path = os.path.join(PROCESSED_DIR, output_filename)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        logger.info(f\"Skipping {filename} — already processed.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Processing: {filename}\")\n",
    "\n",
    "    try:\n",
    "        text = extract_text(file_path)\n",
    "        parsed_data = parse_resume(text)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        logger.info(f\"Successfully processed: {filename} -> {output_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9963154",
   "metadata": {},
   "source": [
    "Section to Handle the Data into a Warehouse to serve to Front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warehosue Pipeline Section\n",
    "\n",
    "from utils.db import get_connection, drop_and_create_tables \n",
    "import os\n",
    "import pandas as pd\n",
    "conn = get_connection()\n",
    "drop_and_create_tables(conn)\n",
    "conn.close()\n",
    "\n",
    "logger.info(\"Warehouse database initialized at data/db/warehouse.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45285f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.db import load_json, get_connection, insert_parsed, insert_candidate, insert_experience, insert_education, insert_skill, insert_to_fts, update_filter_values_for_candidate, insert_quality_score\n",
    "from utils.data_validator import validate_resume_data, save_validation_report, calculate_completeness_score\n",
    "import os\n",
    "\n",
    "files = [f for f in os.listdir(RAW_DIR)]\n",
    "conn = get_connection()\n",
    "\n",
    "for filename in files:\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    parsed_path = os.path.join(PROCESSED_DIR, f\"{base_name}_parsed.json\")\n",
    "    summary_path = os.path.join(SUMMARY_DIR, f\"{base_name}_summary.json\")\n",
    "    resume_path = os.path.join(RAW_DIR, filename)\n",
    "\n",
    "    logger.info(f\"Processing candidate: {base_name}\")\n",
    "\n",
    "    # --- Load parsed + summary files ---\n",
    "    parsed_data = load_json(parsed_path)\n",
    "    summary_data = load_json(summary_path)\n",
    "\n",
    "    try:\n",
    "        parsed_id = insert_parsed(conn, parsed_data, summary_data.get(\"name\"), resume_path)\n",
    "\n",
    "        candidate_id = insert_candidate(conn, summary_data, parsed_id, resume_path)\n",
    "        #insert to respective filter tables\n",
    "        for exp in parsed_data.get(\"experiences\", []):\n",
    "            insert_experience(conn, candidate_id, exp)\n",
    "\n",
    "        for edu in parsed_data.get(\"education\", []):\n",
    "            insert_education(conn, candidate_id, edu)\n",
    "\n",
    "        skills = summary_data.get(\"top_skills\", [])\n",
    "        for skill in skills:\n",
    "            insert_skill(conn, candidate_id, skill)\n",
    "\n",
    "        insert_to_fts(conn, candidate_id, parsed_data, summary_data)\n",
    "\n",
    "        update_filter_values_for_candidate(conn, candidate_id, summary_data, parsed_data)\n",
    "\n",
    "        completeness_score, completeness_grade, missing_required, missing_optional = calculate_completeness_score(parsed_data, summary_data)\n",
    "        logger.info(f\"Completeness: {completeness_score}% (Grade: {completeness_grade})\")\n",
    "\n",
    "        issues = validate_resume_data(parsed_data, summary_data)\n",
    "        \n",
    "        total_issues = len(issues[\"critical\"]) + len(issues[\"formatting\"]) + len(issues[\"warnings\"])\n",
    "        logger.info(f\"Validation: {total_issues} total issues - Critical: {len(issues['critical'])}, Formatting: {len(issues['formatting'])}, Warnings: {len(issues['warnings'])}\")\n",
    "        \n",
    "        insert_quality_score(\n",
    "            conn,\n",
    "            candidate_id,\n",
    "            completeness_score,\n",
    "            completeness_grade,\n",
    "            total_issues,\n",
    "            issues,\n",
    "            missing_required,\n",
    "            missing_optional\n",
    "        )\n",
    "        logger.info(f\"Saved quality score to database\")\n",
    "\n",
    "        save_validation_report(\n",
    "            summary_data.get(\"name\"),\n",
    "            candidate_id,\n",
    "            issues,\n",
    "            parsed_data,\n",
    "            summary_data,\n",
    "            completeness_score,\n",
    "            completeness_grade,\n",
    "            missing_required,\n",
    "            missing_optional\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Successfully inserted candidate: {summary_data.get('name')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to insert {base_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.logger.info_exc()\n",
    "\n",
    "conn.close()\n",
    "logger.info(\"Warehouse ingestion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH='data/db/warehouse.db'\n",
    "import sqlite3\n",
    "\n",
    "# Helper function to display table data in a readable format\n",
    "def show_table_preview(conn, table_name, limit=5):\n",
    "    \"\"\"logger.infos a preview of a table with Pandas for readability.\"\"\"\n",
    "    logger.info(f\"\\nTable: {table_name}\")\n",
    "    try:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT {limit}\", conn)\n",
    "        if df.empty:\n",
    "            logger.info(\"No rows found.\")\n",
    "        else:\n",
    "            logger.info(df.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Failed to query {table_name}: {e}\")\n",
    "\n",
    "def inspect_warehouse():\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        logger.info(f\"Database not found at {DB_PATH}\")\n",
    "        return\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    logger.info(\"🔍 Inspecting warehouse database...\\n\")\n",
    "\n",
    "    # Get all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    if not tables:\n",
    "        logger.info(\"No tables found in warehouse.\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    for table in tables:\n",
    "        show_table_preview(conn, table)\n",
    "\n",
    "    conn.close()\n",
    "    logger.info(\"\\nInspection complete.\")\n",
    "inspect_warehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd569b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e61be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
